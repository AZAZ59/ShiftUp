{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-06T18:18:29.633573Z",
     "start_time": "2017-04-06T22:18:19.198443+04:00"
    },
    "cell_style": "center",
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "['тест']\n",
      "['тест']\n",
      "['тест']\n",
      "['тест']\n",
      "env: PYRO_SERIALIZERS_ACCEPTED=marshal,json,serpent,pickle\n",
      "env: PYRO_SERIALIZER=pickle\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "from pprint import pprint\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "import gensim\n",
    "import pickle\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "import re\n",
    "from sklearn import feature_extraction\n",
    "import warnings\n",
    "import time\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='LDA_Train.log',format='%(levelname)-8s [%(asctime)s] (%(module)s : %(funcName)s) %(message)s',level=logging.INFO)\n",
    "logging.info(\"application started\")\n",
    "logging.info(\"\")\n",
    "logging.info(\"\")\n",
    "\n",
    "# будем отображать графики прямо в jupyter'e\n",
    "%pylab inline\n",
    "\n",
    "#увеличим дефолтный размер графиков\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc=rcParams)\n",
    "sns.set_style(\"darkgrid\", {\n",
    "    \"axes.facecolor\": \"#282828\",\n",
    "    \"axes.edgecolor\":\"#282828\",\n",
    "    \"grid.color\":\"#298F4A\",\n",
    "    \"xtick.color\":\"1\",\n",
    "    \"ytick.color\":\"1\",\n",
    "    \"text.color\":\"1\",\n",
    "    \"axes.labelcolor\":\"1\",\n",
    "    \"figure.facecolor\":\"#282828\"\n",
    "})\n",
    "sns.set_palette(sns.color_palette(\"BrBG\", 7))\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import queue\n",
    "poolS=queue.Queue()\n",
    "\n",
    "THREAD_COUNT=4\n",
    "for i in range(THREAD_COUNT):\n",
    "    m=Mystem(mystem_bin='./mystem',entire_input=False)\n",
    "    print(m.lemmatize(\"тест\"))\n",
    "    poolS.put(m)\n",
    "    \n",
    "%env PYRO_SERIALIZERS_ACCEPTED=marshal,json,serpent,pickle\n",
    "%env PYRO_SERIALIZER=pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-06T18:18:29.651521Z",
     "start_time": "2017-04-06T22:18:29.638980+04:00"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    () a=100\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%timeit() a=100\n",
    "a=np.random.randint(0,100,size=(100,100))\n",
    "b=np.random.randint(0,100,size=(100,100))\n",
    "c=a@b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-06T18:18:29.652452Z",
     "start_time": "2017-04-06T18:18:22.265Z"
    },
    "cell_style": "center",
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    m=poolS.get()\n",
    "    lem=m.lemmatize(text)\n",
    "    poolS.put(m)\n",
    "    return lem\n",
    "stopwords =\"я а е и ж м о на не ни об но он мне мои мож она они оно мной много многочисленное многочисленная многочисленные многочисленный мною мой мог могут можно может можхо мор моя моё мочь над нее оба нам нем нами ними мимо немного одной одного менее однажды однако меня нему меньше ней наверху него ниже мало надо один одиннадцать одиннадцатый назад наиболее недавно миллионов недалеко между низко меля нельзя нибудь непрерывно наконец никогда никуда нас наш нет нею неё них мира наша наше наши ничего начала нередко несколько обычно опять около мы ну нх от отовсюду особенно нужно очень отсюда в во вон вниз внизу вокруг вот восемнадцать восемнадцатый восемь восьмой вверх вам вами важное важная важные важный вдали везде ведь вас ваш ваша ваше ваши впрочем весь вдруг вы все второй всем всеми времени время всему всего всегда всех всею всю вся всё всюду г\\tгод говорил говорит года году где да ее за из ли же им до по ими под иногда довольно именно долго позже более должно пожалуйста значит иметь больше пока ему имя пор пора потом потому после почему почти посреди ей два две двенадцать двенадцатый двадцать двадцатый двух его дел или без день занят занята занято заняты действительно давно девятнадцать девятнадцатый девять девятый даже алло жизнь далеко близко здесь дальше для лет зато даром первый перед затем зачем лишь десять десятый ею её их бы еще при был про процентов против просто бывает бывь если люди была были было будем будет будете будешь прекрасно буду будь будто будут ещё пятнадцать пятнадцатый друго другое другой другие другая других есть пять быть лучше пятый к ком конечно кому кого когда которой которого которая которые который которых кем каждое каждая каждые каждый кажется как какой какая кто кроме куда кругом с\\tт у я та те уж со то том снова тому совсем того тогда тоже собой тобой собою тобою сначала только уметь тот тою хорошо хотеть хочешь хоть хотя свое свои твой своей своего своих свою твоя твоё раз уже сам там тем чем сама сами теми само рано\\\n",
    "самом самому самой самого семнадцать семнадцатый самим самими самих саму семь чему раньше сейчас чего сегодня себе тебе сеаой человек разве теперь себя тебя седьмой спасибо слишком так такое такой такие также такая сих тех чаще четвертый через часто шестой шестнадцать шестнадцатый шесть четыре четырнадцать четырнадцатый сколько сказал сказала сказать ту ты три эта эти что это чтоб этом этому этой этого чтобы этот стал туда этим этими рядом тринадцать тринадцатый этих третий тут эту суть чуть тысяч\"\n",
    "stopwords +=\" about after ago all almost along also any anybody anywhere are arent around ask been before being between but came can cant come could couldnt did didnt does doesnt dont each either else even every everybody everyone find for from get going gone got had has have havent having her here hers him his how ill into isnt its ive just know less like make many may more most much must near never none nothing now off often once one only other our ours out over please rather really said see she should small some something sometime somewhere take than thank thanks that thats the their theirs them then there these they thing think this those though through thus too true two under until upon use very want was way well were what when where which who whom whose why will with within without would yes yet you your yours без более больше большой был была были было быть вам вас вдоль ведь весь видно вместо вне вниз внизу внутри вокруг восемь вот все всегда всего всей всех всё вся где говорил говорили говорим говорить говорят год давай давать давно даже далеко два девять день десять для долго достаточно другого другой другое другие его если есть еще ещё зачем здесь знать ибо изо или именно иметь иной ином каждый каждого каждому каждым как какой какое когда который кроме кто либо лишь между меня мне много мог могу может мои мой навсегда над надо назад нам нас наш него недавно нее неё некто нельзя несколько нет никто них ноль оба обо один однако около она они оно оный опять особенно ото отчего очень под пожалуйста после потому похоже почему почти при про прямо пусть пять равно ребята редко сам самая сами самим самой самому самый самым самому свой себя семь сказал сказали сказать сначала снова совсем спасибо сразу среди стал стала стали стать ста сто так также такие такой там твой тем теперь тогда того тоже той только том тот точно три тут тысяч тысяча тысячи тысячу уже хотя хоть час часто часу чего чей чем четыре что чтоб чтобы чья чье чьё чья шесть эта эти это этого этом этот\"\n",
    "stopwords +=\" a б в г е ж з м т у я є і аж ви де до за зі ми на не ну нх ні по та ти то ту ті це цю ця ці чи ще що як їй їм їх її або але ало без був вам вас ваш вже все всю вся від він два дві для ким мож моя моє мої міг між мій над нам нас наш нею неї них ніж ній ось при про під пір раз рік сам сих сім так там теж тим тих той тою три тут хоч хто цей цим цих час щоб яка які адже буде буду будь була були було бути вами ваша ваше ваші весь вниз вона вони воно всею всім всіх втім геть далі двох день дуже зате його йому каже кого коли кому крім куди лише люди мало мати мене мені миру мною може нами наша наше наші ними ніби один поки пора рано року році сама саме саму самі свою своє свої себе собі став суть така таке такі твоя твоє твій тебе тими тобі того тоді тому туди хоча хіба цими цієї часу чого чому який яких якої якщо ім'я інша інше інші буває будеш більш вгору вміти внизу вісім давно даром добре довго друго дякую життя зараз знову какая кожен кожна кожне кожні краще ледве майже менше могти можна назад немає нижче нього однак п'ять перед поруч потім проти після років самим самих самій свого своєї своїх собою справ такий також тепер тисяч тобою треба трохи усюди усіма хочеш цього цьому часто через шість якого іноді інший інших багато будемо будете будуть більше всього всьому далеко десять досить другий дійсно завжди звідси зовсім кругом кілька людина можуть навіть навіщо нагорі небудь низько ніколи нікуди нічого обидва одного однієї п'ятий перший просто раніше раптом самими самого самому сказав скрізь сьомий третій тільки хотіти чотири чудово шостий близько важлива важливе важливі вдалині восьмий говорив дев'ять десятий зайнята зайнято зайняті занадто значить навколо нарешті нерідко повинно посеред початку пізніше сказала сказати скільки спасибі частіше важливий двадцять дев'ятий зазвичай зайнятий звичайно здається найбільш не можна недалеко особливо потрібно спочатку сьогодні численна численне численні відсотків двадцятий звідусіль мільйонів нещодавно прекрасно четвертий численний будь ласка дванадцять одинадцять сімнадцять тринадцять безперервно дванадцятий одинадцятий одного разу п'ятнадцять сімнадцятий тринадцятий шістнадцять вісімнадцять п'ятнадцятий чотирнадцять шістнадцятий вісімнадцятий дев'ятнадцять чотирнадцятий дев'ятнадцятий\"\n",
    "stopwords = set(stopwords.split(\" \"))\n",
    "stopwords.update(nltk.corpus.stopwords.words('russian'))\n",
    "stopwords.update(nltk.corpus.stopwords.words('english'))\n",
    "stopwords=set(lemmatize(' '.join(stopwords)))\n",
    "\"stopwrds count: %s\"%len(stopwords)\n",
    "# list(stopwords)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-06T18:18:29.653122Z",
     "start_time": "2017-04-06T18:18:23.588Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocessText(text):\n",
    "    m = Mystem(mystem_bin='./mystem',entire_input=False)\n",
    "    text=text.lower()\n",
    "    text=re.sub(r\"&.{1,10}?;\",\"&\",text)\n",
    "    text=re.sub(r\"br\",\"\",text) #remove most common html tag\n",
    "#     print(\"!!!!\",text)\n",
    "    text=re.sub(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z0-9\\&\\.\\/\\?\\:@\\-_=#])*\",\" \",text)#remove url\n",
    "    text=re.sub(r\"[_ ]\",\" \",text)\n",
    "    text=re.sub(r\"[^А-Яа-яA-zA-z- ]\",\"\",text)\n",
    "    text=re.sub(r\" +\",\" \",text)\n",
    "    text=' '.join(lemmatize(text))\n",
    "    text=list(filter(lambda x:x not in stopwords,text.split(\" \")))\n",
    "    return (text)\n",
    "def prepareCorpus(csvs,dictionary=None,preprocess=True,gzip=False):\n",
    "    texts=[]\n",
    "    \n",
    "    if type(csvs)==type(str()):\n",
    "        csvs=[csvs]        \n",
    "    try:\n",
    "        iter(csvs)\n",
    "        logging.debug('iteration will probably work')\n",
    "    except TypeError:\n",
    "        csvs=[csvs]\n",
    "    if preprocess:\n",
    "        posts=pd.DataFrame(columns=[\"text\"])\n",
    "\n",
    "        logging.info('start  loading texts')\n",
    "        import gzip\n",
    "\n",
    "        for csv in csvs:\n",
    "            posts=posts.append(pd.read_csv(csv,header=None,names=[\"text\"]),ignore_index=True)\n",
    "    #         logging.info(\"non nul rows: %i\",posts.count())\n",
    "        logging.info('texts loaded')\n",
    "\n",
    "        logging.info('preprocessing texts') \n",
    "        from multiprocessing.dummy import Pool as ThreadPool\n",
    "        with ThreadPool(THREAD_COUNT) as pool:\n",
    "            texts=pool.map(lambda x: preprocessText(str(x)[2:-2]),posts.values)\n",
    "            logging.info(texts[:5])\n",
    "            logging.info(posts.values[:5])\n",
    "            logging.info('saving stammed texts')\n",
    "            today=time.localtime()\n",
    "            import gzip\n",
    "            pickle.dump(texts,gzip.open('texts{}-{}-{}.bin.gzip'.format(today[2],today[1],today[0]),'wb'))\n",
    "            logging.info('complete')\n",
    "            logging.info('creating base fro corpus [[word for word in text] for text in texts]')\n",
    "            texts = [[word for word in text] for text in texts]\n",
    "            logging.info('preprocessing texts END')\n",
    "    else:\n",
    "        import gzip\n",
    "        for csv in csvs:\n",
    "            logging.info(csv)\n",
    "            texts+=pickle.load(gzip.open('{}.bin.gzip'.format(csv),'rb'))    \n",
    "#     print(texts[:10])\n",
    "    if(dictionary==None):\n",
    "        logging.info(\"create a Gensim dictionary from the texts\")\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "#         dictionary = corpora.HashDictionary(texts)\n",
    "        logging.info('dictionary length:  %i',len(dictionary))\n",
    "    \n",
    "        logging.info(\"remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\")\n",
    "        dictionary.filter_extremes()\n",
    "        logging.info('dictionary length after filtering:  %i',len(dictionary))\n",
    "    \n",
    "    logging.info(\"convert the dictionary to a bag of words corpus for reference\")\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    logging.info('corpus length:  %i',len(corpus))\n",
    "    \n",
    "    return (corpus,dictionary)\n",
    "def trainModel(ldaModel,corpus):\n",
    "    logging.info(\"learning model START\")\n",
    "    %time ldaModel.update(corpus=corpus)\n",
    "    logging.info(\"learning model END!!!!!\")\n",
    "    \n",
    "    import pickle\n",
    "    logging.info(\"saving model\")\n",
    "    with open('LDA_themes:{}_passes:{}_terms:{}.pickle'.format(ldaModel.num_topics,ldaModel.num_updates,ldaModel.num_terms),'wb') as file:\n",
    "        pickle.dump(ldaModel,file)\n",
    "    logging.info(\"model saved\")\n",
    "    printModel(ldaModel)\n",
    "    \n",
    "def printModel(ldaModel):\n",
    "    topics_matrix = ldaModel.show_topics(formatted=True, num_words=20,num_topics=-1)\n",
    "    for i in topics_matrix:\n",
    "        logging.info(i[0],i[1])\n",
    "        logging.info(\"=\"*50)   \n",
    "        \n",
    "def trainClear(csvs):\n",
    "    logging.info(\"prepare corpus START\")\n",
    "    corpus,dictionary=prepareCorpus(csvs)\n",
    "    logging.info(\"prepare corpus END\")\n",
    "    \n",
    "    import time\n",
    "    today=time.localtime()\n",
    "    pickle.dump(corpus,open('corpus{}-{}-{}.bin'.format(today[2],today[1],today[0]),'wb'))\n",
    "    \n",
    "    logging.info(\"creating model START\")\n",
    "    params={\n",
    "        'num_topics':100,\n",
    "#         'workers':2,\n",
    "        'chunksize':4000,\n",
    "        'passes':10,\n",
    "        'id2word':dictionary,\n",
    "    }\n",
    "    ldaModel=gensim.models.LdaModel(distributed=True,**params) #sometimes slow, but faster than with learning (work in 1 threead)\n",
    "    #(3 minutes on corpus length:  99877)\n",
    "    logging.info(\"creating model END\")\n",
    "    \n",
    "    trainModel(ldaModel,corpus)\n",
    "\n",
    "def updateModel(csvs,model):\n",
    "    import pickle\n",
    "    ldaModel=pickle.load(open('./LDA_themes:100_passes:199754_terms:31793.pickle','rb'))\n",
    "    dictionary=ldaModel.id2word\n",
    "    \n",
    "    logging.info(\"prepare corpus START\")\n",
    "    corpus,dictionary=prepareCorpus(csvs,dictionary=dictionary)\n",
    "    logging.info(\"prepare corpus END\")\n",
    "    pickle.dump(corpus,open('corpus{}.bin'.format(str(csvs)),'wb'))\n",
    "    \n",
    "    trainModel(ldaModel,corpus)\n",
    "    \n",
    "preprocessText(\"смотрbrи — яbr <br>https://vk.com/supercards?mid=7485317&amp;fro&amp;m_id=10104402&amp;loc=s827i78592705 docker-compose — запускать и постгрес и нео теперь можно одной командой — docker-compose up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-06T18:18:36.451243Z",
     "start_time": "2017-04-06T22:18:36.423701+04:00"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "csvs=[\n",
    "    './csv/big-0.csv',\n",
    "    './csv/big-1.csv',\n",
    "    './csv/big-2.csv',\n",
    "    './csv/big-3.csv',\n",
    "    './csv/big-4.csv',\n",
    "    './csv/big-5.csv',\n",
    "    './csv/big-6.csv',\n",
    "    './csv/big-7.csv',\n",
    "    './csv/big-8.csv',\n",
    "    './csv/big-9.csv',\n",
    "    './csv/big-10.csv',\n",
    "    './csv/big-11.csv',\n",
    "    './csv/big-12.csv',\n",
    "    './csv/big-13.csv',\n",
    "    './csv/big-14.csv',\n",
    "    './csv/big-15.csv',\n",
    "    './csv/big-16.csv',\n",
    "    './csv/big-17.csv',\n",
    "    './csv/big-18.csv',\n",
    "    './csv/big-19.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-06T18:18:48.585380Z",
     "start_time": "2017-04-06T22:18:42.558564+04:00"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a03e0ceeb056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcsv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}.bin.gzip'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create a Gensim dictionary from the texts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/azaz/anaconda3/lib/python3.5/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mREAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "dictionary = corpora.Dictionary()\n",
    "##TODO extract module build dictionary\n",
    "# dictionary.add_documents\n",
    "corpus=[]\n",
    "for csv in csvs:\n",
    "    logging.info(csv)\n",
    "    texts=pickle.load(gzip.open('{}.bin.gzip'.format(csv),'rb'))    \n",
    "    logging.info(len(texts))\n",
    "    logging.info(\"create a Gensim dictionary from the texts\")\n",
    "    dictionary.add_documents(texts)\n",
    "\n",
    "    logging.info('dictionary length:  %i',len(dictionary))\n",
    "    logging.info(\"remove extremes (similar to the min/max df step used when crнeating the tf-idf matrix)\")\n",
    "    dictionary.filter_extremes()\n",
    "    logging.info('dictionary length after filtering:  %i',len(dictionary))\n",
    "    \n",
    "    pickle.dump(dictionary,open('{}_dict.bin'.format(csv),'wb'))\n",
    "\n",
    "pickle.dump(dictionary,open('dictionary_10G.bin'.format(csv),'wb'))\n",
    "#     logging.info(\"convert the dictionary to a bag of words corpus for reference\")\n",
    "#     courpus = [dictionary.doc2bow(text) for text in texts]\n",
    "#     logging.info('corpus length:  %i',len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-06T18:41:28.330237Z",
     "start_time": "2017-04-06T22:18:51.711461+04:00"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "##TODO extract module build corpus\n",
    "corpus=[]\n",
    "dictionary = dictionary\n",
    "for csv in csvs:\n",
    "    logging.info(csv)\n",
    "    texts=pickle.load(gzip.open('{}.bin.gzip'.format(csv),'rb'))    \n",
    "    logging.info(len(texts))\n",
    "    logging.info(\"convert the dictionary to a bag of words corpus for reference\")\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    logging.info('corpus length:  %i',len(corpus))\n",
    "    pickle.dump(corpus,open('{}_corpus.bin'.format(csv),'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-05T18:49:19.751Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(corpus,open('corpus.bin'.format(csv),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-05T12:10:51.179Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "csvs=[\n",
    "    './csv/big-0.csv',\n",
    "    './csv/big-1.csv',\n",
    "    './csv/big-2.csv',\n",
    "    './csv/big-3.csv',\n",
    "    './csv/big-4.csv',\n",
    "    './csv/big-5.csv',\n",
    "    './csv/big-6.csv',\n",
    "    './csv/big-7.csv',\n",
    "    './csv/big-8.csv',\n",
    "    './csv/big-9.csv',\n",
    "    './csv/big-10.csv',\n",
    "    './csv/big-11.csv',\n",
    "    './csv/big-12.csv',\n",
    "    './csv/big-13.csv',\n",
    "    './csv/big-14.csv',\n",
    "    './csv/big-15.csv',\n",
    "    './csv/big-16.csv',\n",
    "    './csv/big-17.csv',\n",
    "    './csv/big-18.csv',\n",
    "    './csv/big-19.csv',\n",
    "]\n",
    "logging.info(\"prepare corpus START\")\n",
    "##TODO extract module preprocess\n",
    "for csv in csvs:\n",
    "    logging.info('start %s'%csv)\n",
    "    logging.info('loading ')\n",
    "    posts=pd.read_csv(csv,header=None,names=[\"text\"])\n",
    "    logging.info('load  %s lines'%len(posts))\n",
    "    from multiprocessing.dummy import Pool as ThreadPool\n",
    "    with ThreadPool(THREAD_COUNT) as pool:\n",
    "        \n",
    "        texts=pool.map(lambda x: preprocessText(str(x)[2:-2]),posts.values)\n",
    "        logging.info(texts[:5])\n",
    "        logging.info(posts.values[:5])\n",
    "        logging.info('saving stammed texts ащк')\n",
    "        today=time.localtime()\n",
    "        import gzip\n",
    "        pickle.dump(texts,gzip.open('{}.bin.gzip'.format(csv),'wb'))\n",
    "        logging.info('complete %s'%csv)\n",
    "\n",
    "\n",
    "corpus,dictionary=prepareCorpus(csvs,preprocess=False)\n",
    "import gzip\n",
    "pickle.dump((corpus,dictionary),gzip.open('corp_dict_pair10G.bin.gzip','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-06T17:32:39.496116Z",
     "start_time": "2017-04-06T21:32:38.266656+04:00"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# corpus     = pickle.load(open('corpus.bin','rb'))\n",
    "import pickle\n",
    "dictionary = pickle.load(open('dictionary_10G.bin','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T16:47:13.195997Z",
     "start_time": "2017-03-28T20:45:07.896982+04:00"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "logging.info(\"readData\")\n",
    "today=time.localtime()\n",
    "(corpus,dictionary)=pickle.load(open('corpus{}-{}-{}.bin'.format(today[2],today[1],today[0]),'rb'))\n",
    "logging.info(\"End readData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T17:54:37.448098Z",
     "start_time": "2017-03-28T17:37:43.382Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "logging.info(\"creating model START\")\n",
    "logging.info('courpus Length:'+len(corpus))\n",
    "for i in range(1,100):   \n",
    "    logging.info(\"courrent step:\"+i)\n",
    "    params={\n",
    "        'num_topics':100,\n",
    "        'chunksize':10000,\n",
    "        'passes':10,\n",
    "        'id2word':dictionary,\n",
    "        'corpus':corpus[i*10000:(i+1)*10000],\n",
    "        'update_every':5,\n",
    "        'distributed':True,\n",
    "        'ns_conf':{'host':'25.52.115.42'}\n",
    "    }\n",
    "    # ldaModel=gensim.models.LdaModel(corpus=corpus,distributed=True,id2word=dictionary,chunksize=4000,num_topics=100,passes=10) #sometimes slow, but faster than with learning (work in 1 threead)\n",
    "#     ldaModel = gensim.models.LdaModel(\n",
    "#     **params    \n",
    "#     )\n",
    "#     today=time.localtimerd':dictionary,\n",
    "        'corpus':corpus[i*10000:(i+1)*10000],\n",
    "        'update_every':5,\n",
    "        '()\n",
    "#     pickle.dump(ldaModel,open('model{}-{}-{}:{}.bin'.format(today[2],today[1],today[0],i),'wb'))\n",
    "#(3 minutes on corpus length:  99877)\n",
    "# logging.info(\"creating model END\")\n",
    "# printModel(ldaModel)\n",
    "# trainModel(ldaModel,corpus)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "ShiftUp/LDA.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "ru",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ru",
   "useGoogleTranslate": true
  },
  "notify_time": "5",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "447px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
